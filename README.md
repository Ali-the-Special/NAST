# NAST
# NAST: Negation-Aware Selective Training for Medical Visionâ€“Language Models

Official repository for:

> **NAST: Negation-Aware Selective Training for Medical Visionâ€“Language Models**  
> (ICML submission)

This repository provides:

- âœ… Construction code for the **polarity-controlled diagnostic benchmark**
- âœ… Construction code for the **contextual clinical negation dataset**
- âœ… Causal tracing (CTE) implementation for CLIP-based models
- âœ… Evaluation pipelines for retrieval and claim-ranking tasks

---

## ğŸ” Overview

Medical visionâ€“language models (VLMs) exhibit systematic difficulty in interpreting negation (e.g., *â€œno pneumothoraxâ€*).  
This repository supports reproducible evaluation and analysis of negation sensitivity in medical VLMs.

The project includes:

- A **polarity-controlled diagnostic benchmark** (negated vs affirmative-equivalent MCQs)
- A **contextual negation benchmark** for retrieval and claim-based evaluation
- A **causal tracing framework** for estimating layer-wise negation contribution (CTE)

âš ï¸ This repository does **not** distribute MIMIC-CXR images or raw reports.  
You must obtain access through the official MIMIC-CXR process.

---

## ğŸ“‚ Repository Structure

<pre>
nast-negation-medvlm/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ nast/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”‚
â”‚       â”œâ”€â”€ evaluation/
â”‚       â”œâ”€â”€ causal_tracing/
â”‚       â”œâ”€â”€ models/
â”‚       â””â”€â”€ utils/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_eval_benchmark.py
â”‚   â”œâ”€â”€ build_contextual_dataset.py
â”‚   â”œâ”€â”€ validate_jsonl.py
â”‚   â””â”€â”€ make_splits_patient_level.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ benchmarks/
â”‚   â”‚   â”œâ”€â”€ mednega_cxr_eval/
â”‚   â”‚   â””â”€â”€ contextual_negation/
â”‚   â””â”€â”€ mappings/
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ benchmark.md
â”‚   â”œâ”€â”€ data_card.md
â”‚   â””â”€â”€ causal_tracing.md
â”‚
â””â”€â”€ private/ 
  </pre>

![Affirmativeâ€“Negation Gap Example](assets/figures/evaluation_benchmark.png)
